---
title: "Finding Opportunity in Crisis"
author: "Jens Meydam"
date: "2019-06-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(caret)
library(gbm)
library(ROCR)

rm(list = ls())

# https://stackoverflow.com/questions/8197559/emulate-ggplot2-default-color-palette
RED = "#F8766D"
GREEN = "#00BFC4"

csv_file <- "bank-additional/bank-additional-full.csv"

# Uncomment to download data:

# dl <- tempfile()
# download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip", dl)
# unzip(dl, csv_file)

d <- read.table(csv_file, header = TRUE, sep = ";", stringsAsFactors = TRUE)


# Add y_bernoulli for GBM, otherwise error message:
# "Bernoulli requires the response to be in {0,1}"
d$y_bernoulli <- unclass(d$y) - 1

```

# Preliminary Remark

This project is an exercise. All results are derived from the data set and the 
accompanying documentation.


# Executive Summary

This is an analysis of the [bank marketing data set](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing) 
made available via the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml) 
maintained by the University of California, Irvine. 

The data were collected during direct marketing campaigns run by a Portuguese 
bank. The classification goal is to predict whether a client will accept the 
offer - made via phone call - to invest money in a term deposit.

Note that the data were collected from May 2008 to November 2010, covering the
period from just before until two years into the financial crisis, and that 
term deposits are considered a safe investment.

The date of the observations is not given, but the 41'188 observations are 
ordered by date. There is a sharp drop in the [3 month Euribor interest rate](https://www.investopedia.com/terms/e/euribor.asp) 
aoround observation 25'000.

If we measure the efficiency of campaigns in terms of the relative
frequency of positive responses, campaigns became daramatically more efficient
- and in that sense more successful - starting around observation 27'500.

While the timing suggests that the state of the economy played a role,
the improvement of the success rate might also be due to a learning effect:
the campaigns towards the end may have learned lessons from previous
campaigns and this might be an important reason why later campaigns
were more efficient/successful.

Could the improvement of campaign performance have come earlier, before
the onset of the financial crisis?

At the end of this study, a commonly highly effective machine learning 
algorithm is trained and tested both on the data before the onset of 
the financial crisis and on the data after, excluding as far as possible
information on the state of the economy. 

The model trained on the earlier data has no predictive power, which 
suggests that the data contain no useful signal. 

In contrast, the model trained on the later data has considerable 
predictive power and could have been used for optimizing campaigns. 
Indeed, later campaigns appear to have been informed by such a model, 
systematically targeting subgroups and calibrating the number of calls.

Our claim is not so much that the earlier data are in fact completely 
useless, but that the later data are strikingly more useful. Dramatic
changes in the economy led to the amplification of signals in the data 
and/or to the emergence of entirely new signals. We suggest this as 
a potentially fruitful avenue for further research.

To conclude, it appears that the marked improvement of campaign performance 
was the result of a new, optimized approach that exploited changing 
attitudes in a country in crisis, and that a similar improvement would 
not have been possible before.

In particular, it is conceivable that certain identifiable subgroups 
among the clients may have become more susceptible to sales pitches 
emphasizing security and wealth protection, but this is mere speculation.


# Data Set

> [bank marketing data set](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing) 
> made available via the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml) 
> maintained by the University of California, Irvine. 

Specific data set used:

> bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date 
> (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014].

Download link:

> <http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip>

Full citation for data set: 

> [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014

As stated on the website, the data were collected during

> [...] direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe [to] a term deposit (variable y).

Background information on term deposits:

> <https://www.investopedia.com/terms/t/termdeposit.asp>

> A term deposit is a fixed-term investment that includes the deposit of money into an account at a financial institution. Term deposit investments usually carry short-term maturities ranging from one month to a few years and will have varying levels of required minimum deposits.
> 
> The investor must understand when buying a term deposit that they can withdraw their funds only after the term ends. In some cases, the account holder may allow the investor early termination—or withdrawal—if they give several days notification. Also, there will be a penalty assessed for early termination.
>
> [...] Term deposits are an extremely safe investment and are therefore very appealing to conservative, low-risk investors.

> <http://archive.ics.uci.edu/ml/datasets/Bank+Marketing>

## Attributes from Bank Client Data

1. age (numeric)
2. job: type of job (categorical: "admin.", "blue-collar", "entrepreneur", "housemaid", "management", "retired", "self-employed", "services", "student", "technician", "unemployed", "unknown")
3. marital: marital status (categorical: "divorced", "married", "single", "unknown"; note: "divorced" means divorced or widowed)
4. education (categorical: "basic.4y", "basic.6y", "basic.9y", "high.school", "illiterate", "professional.course", "university.degree", "unknown")
5. default: has credit in default? (categorical: "no", "yes", "unknown")
6. housing: has housing loan? (categorical: "no", "yes", "unknown")
7. loan: has personal loan? (categorical: "no", "yes", "unknown")

## Attributes Related to the Last Contact of the Current Campaign

8. contact: contact communication type (categorical: "cellular", "telephone") 
9. month: last contact month of year (categorical: "jan", "feb", "mar", ..., "nov", "dec")
10. day_of_week: last contact day of the week (categorical: "mon", "tue", "wed", "thu", "fri")
11. duration: last contact duration, in seconds (numeric). **Important note**:  this attribute highly affects the output target (e.g., if duration=0 then y="no"). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.

## Other Attributes

12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
14. previous: number of contacts performed before this campaign and for this client (numeric)
15. poutcome: outcome of the previous marketing campaign (categorical: "failure", "nonexistent", "success")

## Social and Economic Context Attributes

16. emp.var.rate: employment variation rate - quarterly indicator (numeric)
17. cons.price.idx: consumer price index - monthly indicator (numeric)     
18. cons.conf.idx: consumer confidence index - monthly indicator (numeric)     
19. euribor3m: euribor 3 month rate - daily indicator (numeric)
20. nr.employed: number of people employed - quarterly indicator (numeric)

## Output Variable (Target)

21. y: has the client subscribed a term deposit? (binary: "yes", "no")

Added for GBM: y_bernoulli (0 = "no", 1 = "yes")


# Exploratory Data Analysis

## Social and Economic Context Attributes

Most of these variables are correlated. 

The date of the 41'188 observations observations is not given, but
are ordered by date, from May 2008 to November 2010. Therefore, 
plotting the data in sequence will give an approximation 
to a proper time series.

There is a sharp drop in the [3 month Euribor interest rate](https://www.investopedia.com/terms/e/euribor.asp) 
aoround observation 25'000.

```{r, echo=FALSE}
d %>% mutate(observation = 1:n()) %>%
  ggplot(aes(observation, euribor3m)) + 
  geom_line() +
  ggtitle("3 Month Euribor Interest Rate")
```

Note:

- The observations cover the period from just before to two years 
  after the onset of the global financial crisis. 
  Lehman Brothers collapsed on September 15, 2008.
  (https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%932008)
- The observations are not evenly distributed in time. 
- Higher variability after observation 36,000 is partly due 
  to fewer observations per time unit (fewer/lower intensity campaigns)
  
In the rest of this section we will focus on the the number of people 
employed. Note the solid downward trend after the onset of the crisis:  

```{r, echo=FALSE}
d %>% mutate(observation = 1:n()) %>%
  ggplot(aes(observation, nr.employed)) + 
  geom_line() +
  ggtitle("Number of People Employed")
```

Now we will also show the responses ("yes"/"no"). We will
show responses as jittered points to make gradual changes in 
frequency easier to see. 

```{r, echo=FALSE}
d %>% mutate(observation = 1:n()) %>% 
  ggplot(aes(x = observation, y = nr.employed, color = y)) + 
  geom_point(position = position_jitter(width = 0, height = 20),
             alpha = 0.4) +
  ggtitle("Number of People Employed, With Response")
```

Note how "yes" responses become more dominant towards the end.
Keep in mind that observations are not evenly distributed in 
time. The number of people employed is a quaterly indicator; 
apparent accelerated change towards the end is an effect
of having fewer observations per time unit.  

While there are "yes" responses throughout, the "no" responses 
appear to drown out the "yes" responses for most of the plot. 
Next, to see trends for both responses more clearly, we will plot 
density curves of "yes" and "no" responses. These curves are 
smoothed and normalized so that each curve integrates to 1.

```{r, echo=FALSE}
d %>% mutate(observation = 1:n()) %>%
  ggplot(aes(x = observation, color = y)) + 
  geom_line(stat = "density") +
  ggtitle("Changing Freqeuncy of Responses as Densities")
```

Note the slow, then halting increase in the frequency of "yes" 
responses before the financial crisis, followed by steep bump 
just before observation 30,000, and then a solid rise (the drop 
at the end is an artifact of the plot). 

Next, to show the changing share of "yes" and "no" responses,
we will bin observations in groups of 2,500.

```{r, echo=FALSE}
d %>% mutate(observation = 1:n()) %>%
  mutate(group = (observation %/% 2500) * 2500) %>%
  group_by(group) %>%
  ggplot(aes(x = group, fill = y)) + 
  geom_bar(position = position_stack()) +
  ggtitle("Changing Share of Responses")
```

Note the much lower prevalence of "yes" responses in the first two thirds
of the plot. Again, the lower bar at the end and consequently the apparent 
drop both of "yes" and "no" responses is an artifact of the plot. In fact, 
the share of "yes" responses rises to over 50% in the last bar.

### Thoughts

In the first visual displays of the data, until about observation 36,000,
the "no" responses drowned out the "yes" responses  - even using high
levels of jitter and transparency left some uncertainty concerning
the changing absolute frequency of "yes" responses. After observation 
36,000 the relative frequency of "no" responses drop dramatically, 
making the "yes" responses clearly visible.

Further analysis confirmed that "yes" responses, while present from the
beginning, became much more frequent starting around observation 27'500,
and ended up being more frequent than "no" responses.

If we measure the efficiency of the campaigns in terms of the ratio of
"yes" to "no" responses, campaigns became daramatically more efficient
- and in that sense more successful - starting around observation 27'500.

The most efficient campaigns towards the end can also be identified via
nr.employed, since the number of people employed is decreasing for the second
half of the observations and is significantly lower towards the end.
The employment variation rate (emp.var.rate) can also be used to detect
observations from 27'500 onwards.

There might be a causal connection between the efficiency / success
of campaigns and a combination of economic indicators (in particular,
the number of people employed and the employment variation rate), but the
improvement of the success rate might also be due to a learning effect:
the campaigns towards the end may have learned lessons from previous
campaigns and this might be an important reason why later campaigns
were more efficient/successful.

It is worth noting that after an initial dramatic improvement around
observation 27'500 performance first went back to the previous level,
then returned to the improved level, and then improved even further.
It is conceivable that this discontinuity and further accelerated
improvement is a result of different campaigns using different
approaches, with systematic optimization of the approach after a
brief delay after a first breakthrough around observation 27'500.
This new, optimized approach might have exploited changing attitudes
in a country in crisis. In particular, it is conceivable that certain
identifiable subgroups among the customers may have become more susceptible
to sales pitches emphasizing security and wealth protection.


## Other Attributes

As we will see later, three attributes are particularly relevant:

1. Job
2. Age
3. Education

We will show graphs for these three attributes, contrasting the frequencies
of the various values "before" and "after", while also showing the response y.

Note the changes after the onset of the financial crisis. In general,
the share of positive responses has increased. Categories and ranges with
relatively many positive responses show a marked increase, some more so 
than others. Some categories and ranges that were previously hardly 
represented or not represented at all become noticeable, with very high 
shares of positive responses, in particuar the categories "student" 
and "retired" and the associated age ranges.

```{r, echo=FALSE}
d %>%
  mutate(time = ifelse(row_number() <= 27500, "before", "after")) %>%
  mutate(time = factor(time, levels = c("before", "after"))) %>%
  group_by(job) %>% 
  mutate(count = n()) %>%
  ggplot(aes(x = reorder(job, -count), fill = y)) + 
  geom_bar() + 
  xlab("job") + 
  ylab("") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~time, ncol = 1) +
  ggtitle("Job")
```

```{r, echo=FALSE}
d %>%
  mutate(time = ifelse(row_number() <= 27500, "before", "after")) %>%
  mutate(time = factor(time, levels = c("before", "after"))) %>%
  ggplot(aes(x = age, fill = y)) + 
  geom_histogram(bins = 20) + 
  xlab("job") + 
  ylab("") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~time, ncol = 1)f +
  ggtitle("Age")
```

```{r, echo=FALSE}
d %>%
  mutate(time = ifelse(row_number() <= 27500, "before", "after")) %>%
  mutate(time = factor(time, levels = c("before", "after"))) %>%
  group_by(education) %>% 
  mutate(count = n()) %>%
  ggplot(aes(x = reorder(education, -count), fill = y)) + 
  geom_bar() + 
  xlab("education") + 
  ylab("") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~time, ncol = 1) +
  ggtitle("Education")
```

### Thoughts

A general effect of the financial crisis is highly likely. However,
the emergence of new groups with high success rates points to changes
in the targeting of the campaigns. 


# Model

Given the obvious changes both in the economy and in the targeting of the
campaigns after the onset of the financial crisis, a model based on the whole
data set is bound to give misleading results. The data set is therefore split
into two data sets, "before" and "after" the onset of the financial crisis,
using observation 27,500 as the cut-off. Each of these new data sets is then
split into a training and a test set.

```{r, include=FALSE}
# index of observations in orginal data set before and after observation 27,500
index_before <- 1:27500
index_after <- 27501:nrow(d)
length(index_before)
length(index_after)
length(index_before) + length(index_after) == nrow(d)

# Splitting orginal data set into "before" and "after" set
train_test_before <- d[index_before,]
train_test_after <- d[index_after,]

# new training and test set "before"

# test set to be used for model selection, 10% of remaining data
test_index_before <- createDataPartition(y = train_test_before$y, 
                                         times = 1, p = 0.1, 
                                         list = FALSE)
train_before <- train_test_before[-test_index_before,]
test_before <- train_test_before[test_index_before,]

nrow(test_before) / (nrow(test_before) + nrow(train_before))

# new training and test set "after"

# test set to be used for model selection, 10% of remaining data
test_index_after <- createDataPartition(y = train_test_after$y, 
                                        times = 1, p = 0.1, 
                                        list = FALSE)
train_after <- train_test_after[-test_index_after,]
test_after <- train_test_after[test_index_after,]

nrow(test_after) / (nrow(test_after) + nrow(train_after))

nrow(d) == nrow(train_before) + nrow(test_before) + 
           nrow(train_after) + nrow(test_after)
```

Since there are many categorical variables and at least some effects of
continuous variables (age!) may be nonlinear, a tree-based model is a natural
choice. We will fit a simple boosted model using the 
[gbm package](https://cran.r-project.org/web/packages/gbm/index.html).

The emphasis here is on understanding what is roughly possible with the
data before and after the onset of the financial crisis, while leaving out all 
economic variables and the month to reduce timing effects. 

Tuning the models by optimizing paramters would be a natural next step.

## "Before"

```{r, include=FALSE}
set.seed(5)
```

```{r}
boost_before_3 <- gbm(y_bernoulli ~ ., 
                      data = subset(train_before, 
                                    select = -c(duration, month, euribor3m, emp.var.rate, 
                                                cons.price.idx, cons.conf.idx, nr.employed,
                                                y)),
                      distribution = "bernoulli", 
                      n.trees = 5000, 
                      interaction.depth = 4)

summary(boost_before_3)
#                     var      rel.inf
# job                 job 27.152056729
# age                 age 19.571323650
# education     education 16.539990870
# day_of_week day_of_week 13.536900141
# campaign       campaign  7.795835207
# marital         marital  4.682135452
# contact         contact  3.290501427
# housing         housing  2.764413041
# loan               loan  2.445246409
# default         default  1.525721577
# poutcome       poutcome  0.506492675
# previous       previous  0.181584149
# pdays             pdays  0.007798672
```

### test_before

```{r}
yhat_boost_before <- predict(boost_before_3,
                             type = "response",
                             newdata = test_before,
                             n.trees = 5000)

summary(test_before$y_bernoulli == 1)
summary(yhat_boost_before)
# using cut-off for probability to predict a reasonable number of true "yes"
summary(yhat_boost_before > 0.05)
actual_before <- ifelse(test_before$y_bernoulli == 1, "yes", "no")
predicted_before <- ifelse(yhat_boost_before > 0.05, "yes", "no")
mean(actual_before == predicted_before)
table(actual_before, predicted_before)

plot(performance(prediction(predictions = yhat_boost_before, 
                            labels = ifelse(test_before$y_bernoulli == 1, "yes", "no")), 
                 "tpr", 
                 "fpr"), 
     print.cutoffs.at = c(0.05, 0.10, 0.15),
     main = "GBM, Before")
```

This model is basically useless. Inspecting the ROC curve shows that the model performs
at the level of random guessing.

## "After"


```{r}

boost_after_3 <- gbm(y_bernoulli ~ ., 
                      data = subset(train_after, 
                                    select = -c(duration, month, euribor3m, emp.var.rate, 
                                                cons.price.idx, cons.conf.idx, nr.employed,
                                                y)),
                      distribution = "bernoulli", 
                      n.trees = 5000, 
                      interaction.depth = 4)

summary(boost_after_3)
#                     var    rel.inf
# job                 job 24.7270879
# age                 age 21.2481151
# education     education 13.6982947
# day_of_week day_of_week 11.4210767
# pdays             pdays  8.8720524
# campaign       campaign  4.7749730
# poutcome       poutcome  3.5969610
# marital         marital  3.3450231
# housing         housing  2.4191063
# previous       previous  2.3089277
# loan               loan  1.8543587
# contact         contact  0.9871851
# default         default  0.7468385
```

### test_after

```{r}
yhat_boost_after <- predict(boost_after_3,
                             type = "response",
                             newdata = test_after,
                             n.trees = 5000)

summary(test_after$y_bernoulli == 1)
summary(yhat_boost_after)
# using cut-off for probability to predict a reasonable number of true "yes"
summary(yhat_boost_after > 0.20)
actual_after <- ifelse(test_after$y_bernoulli == 1, "yes", "no")
predicted_after <- ifelse(yhat_boost_after > 0.20, "yes", "no")
mean(actual_after == predicted_after)
table(actual_after, predicted_after)

plot(performance(prediction(predictions = yhat_boost_after, 
                            labels = ifelse(test_after$y_bernoulli == 1, "yes", "no")), 
                 "tpr", 
                 "fpr"), 
     print.cutoffs.at = c(0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35),
     main = "GBM, After")
```

This model clearly has predictive power. A cut-off can be chosen to calibrate
campaigns, for example to improve the proportion of positive responses
or to reach a target number of positive responses.


## Results

A commonly highly effective machine learning algorithm was trained and 
tested both on the data before the onset of the financial crisis and 
on the data after, excluding as far as possible information on the 
state of the economy. 

The model trained on the earlier data has no predictive power, which 
suggests that the data contain no useful signal. 

In contrast, the model trained on the later data has considerable 
predictive power and could have been used for optimizing campaigns. 
Indeed, as pointed out earlier, later campaigns appear to have been  
systematically targeting subgroups. It can be assumed that later 
campaigns were based on models similar to the one presented here.

Our claim is not so much that the earlier data are in fact completely 
useless, but that the later data are strikingly more useful. Dramatic
changes in the economy led to the amplification of signals in the data 
and/or to the emergence of entirely new signals. We suggest this as 
a potentially fruitful avenue for further research.


# Conclusion

It appears that the marked improvement of campaign performance 
was the result of a new, optimized approach that exploited changing 
attitudes in a country in crisis, and that a similar improvement would 
not have been possible before.

In particular, it is conceivable that certain identifiable subgroups 
among the clients may have become more susceptible to sales pitches 
emphasizing security and wealth protection, but this is mere speculation.

